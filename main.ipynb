{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from abc_py.interface import ABC\n",
    "import functools\n",
    "import os\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_actions = [\n",
    "    functools.partial(ABC.resub, zero_cost=False, preserve_levels=False),\n",
    "    functools.partial(ABC.resub, zero_cost=False, preserve_levels=True),\n",
    "    functools.partial(ABC.resub, zero_cost=True, preserve_levels=False),\n",
    "    functools.partial(ABC.resub, zero_cost=True, preserve_levels=True),\n",
    "    functools.partial(ABC.rewrite, zero_cost=False, preserve_levels=True, verbose=False),\n",
    "    functools.partial(ABC.rewrite, zero_cost=True, preserve_levels=True, verbose=False),\n",
    "    functools.partial(ABC.rewrite, zero_cost=False, preserve_levels=False, verbose=False),\n",
    "    functools.partial(ABC.rewrite, zero_cost=True, preserve_levels=False, verbose=False),\n",
    "    functools.partial(ABC.refactor, zero_cost=False, preserve_levels=True),\n",
    "    functools.partial(ABC.refactor, zero_cost=False, preserve_levels=False),\n",
    "    functools.partial(ABC.refactor, zero_cost=True, preserve_levels=True),\n",
    "    functools.partial(ABC.refactor, zero_cost=True, preserve_levels=False),\n",
    "    functools.partial(ABC.balance),\n",
    "]\n",
    "\n",
    "num_actions = len(possible_actions)\n",
    "num_features = 8\n",
    "learning_rate = 0.01\n",
    "discount_factor = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, num_features, num_actions):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_features, 20)\n",
    "        self.fc2 = nn.Linear(20, 20)\n",
    "        self.fc3 = nn.Linear(20, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.softmax(self.fc3(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_features, 10)\n",
    "        self.fc2 = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(actor: ActorNetwork, state: torch.Tensor):\n",
    "    action_probs = actor(state)\n",
    "    action_distribution = Categorical(action_probs)\n",
    "    action = action_distribution.sample()\n",
    "    return action.item(), action_distribution.log_prob(action).reshape(1), action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_losses(critic: CriticNetwork, action_log_prob: torch.Tensor, reward: float, state: torch.Tensor, next_state: torch.Tensor):\n",
    "    value = critic(state)\n",
    "    next_value = critic(next_state)\n",
    "    td_error = reward + discount_factor * next_value - value\n",
    "    actor_loss = -action_log_prob * td_error.detach()\n",
    "    critic_loss = td_error ** 2\n",
    "    return actor_loss, critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reward(state: torch.Tensor, next_state: torch.Tensor):\n",
    "    old_levels, old_nodes = state[4], state[5]\n",
    "    new_levels, new_nodes = next_state[4], next_state[5]\n",
    "    delay_estimate = new_levels / old_levels\n",
    "    area_estimate = new_nodes / old_nodes\n",
    "    reward = 0\n",
    "\n",
    "    if delay_estimate < 1:\n",
    "        if area_estimate < 1:\n",
    "            reward = 3\n",
    "        elif area_estimate == 1:\n",
    "            reward = 2\n",
    "        else:\n",
    "            reward = 1\n",
    "    elif delay_estimate == 1:\n",
    "        if area_estimate < 1:\n",
    "            reward = 2\n",
    "        elif area_estimate == 1:\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = -2\n",
    "    else:\n",
    "        if area_estimate < 1:\n",
    "            reward = 1\n",
    "        elif area_estimate == 1:\n",
    "            reward = -2\n",
    "        else:\n",
    "            reward = -3\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(actor: ActorNetwork, critic: CriticNetwork, actor_optimizer, critic_optimizer, episodes=50, iterations=50):\n",
    "    for episode in range(episodes):\n",
    "        total_actor_loss = 0\n",
    "        total_critic_loss = 0\n",
    "\n",
    "        for dir in os.listdir(\"./benchmarks\"):\n",
    "            if dir != \"arithmetic\":\n",
    "                continue\n",
    "            \n",
    "            for filename in os.listdir(f\"./benchmarks/{dir}\"):\n",
    "                if not filename.endswith(\".aig\"):\n",
    "                    continue\n",
    "\n",
    "                abc = ABC()\n",
    "                init_stats = abc.read_aiger(f\"./benchmarks/{dir}/{filename}\")\n",
    "                assert len(init_stats) == num_features\n",
    "\n",
    "                state = torch.tensor(init_stats[:2] + [1] * (num_features - 2), dtype=torch.float)\n",
    "                init_stats = torch.tensor(init_stats, dtype=torch.float)\n",
    "\n",
    "                for i in range(iterations):\n",
    "                    # select action from actor model\n",
    "                    action, action_log_prob, _ = select_action(actor, state)\n",
    "\n",
    "                    # take action and observe next state\n",
    "                    action_to_be_taken = possible_actions[action]\n",
    "                    new_stats = action_to_be_taken(abc)\n",
    "                    next_state = torch.tensor(new_stats, dtype=torch.float) / init_stats # take ratio with respect to initial stats\n",
    "                    next_state[0] = state[0] # keep the number of inputs same\n",
    "                    next_state[1] = state[1] # keep the number of outputs same\n",
    "\n",
    "                    if init_stats[2] == 0:\n",
    "                        next_state[2] = 0\n",
    "\n",
    "                    # calculate reward and update actor and critic models\n",
    "                    reward = calculate_reward(state, next_state)\n",
    "                    actor_loss, critic_loss = calculate_losses(critic, action_log_prob, reward, state, next_state)\n",
    "                    total_actor_loss += actor_loss.detach().item()\n",
    "                    total_critic_loss += critic_loss.detach().item()\n",
    "\n",
    "                    actor_optimizer.zero_grad()\n",
    "                    actor_loss.backward()\n",
    "                    actor_optimizer.step()\n",
    "                    critic_optimizer.zero_grad()\n",
    "                    critic_loss.backward()\n",
    "                    critic_optimizer.step()\n",
    "\n",
    "                    # update state\n",
    "                    state = next_state\n",
    "\n",
    "        total_actor_loss /= iterations\n",
    "        total_critic_loss /= iterations\n",
    "        print(f\"Episode {episode + 1}: Actor Loss: {total_actor_loss}, Critic Loss: {total_critic_loss}\")\n",
    "        \n",
    "        if abc.quit() != 0:\n",
    "            print(\"Error in quitting abc\")\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = ActorNetwork(num_features, num_actions)\n",
    "critic = CriticNetwork(num_features)\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=learning_rate)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Actor Loss: 0.002944391734402212, Critic Loss: 1.4680209745616548\n",
      "Episode 2: Actor Loss: 5.177456864296701e-08, Critic Loss: 1.3974336014125912\n",
      "Episode 3: Actor Loss: -2.1068808350455104e-08, Critic Loss: 1.3542953108400184\n",
      "Episode 4: Actor Loss: -1.5668985365565118e-08, Critic Loss: 0.9317612949688495\n",
      "Episode 5: Actor Loss: -8.567338237980282e-08, Critic Loss: 1.536460804142838\n",
      "Episode 6: Actor Loss: -6.338802307007119e-08, Critic Loss: 1.3063443870092137\n",
      "Episode 7: Actor Loss: -6.458423523730827e-08, Critic Loss: 1.2414588668119684\n",
      "Episode 8: Actor Loss: -6.047178431979606e-08, Critic Loss: 1.1329745596959864\n",
      "Episode 9: Actor Loss: -4.03291656564061e-08, Critic Loss: 0.9921232892846809\n",
      "Episode 10: Actor Loss: -1.1910036661760825e-09, Critic Loss: 0.8952761254914475\n",
      "Episode 11: Actor Loss: 2.2132935308810663e-08, Critic Loss: 0.8773737777642459\n",
      "Episode 12: Actor Loss: 6.34354469617555e-08, Critic Loss: 1.2192334638896138\n",
      "Episode 13: Actor Loss: 1.0260956356145102e-07, Critic Loss: 1.5079784180148272\n",
      "Episode 14: Actor Loss: 8.914034574453167e-08, Critic Loss: 1.4898468437634143\n",
      "Episode 15: Actor Loss: 7.786291901372075e-08, Critic Loss: 1.4766356278147996\n",
      "Episode 16: Actor Loss: 6.820676038188367e-08, Critic Loss: 1.4667497437691055\n",
      "Episode 17: Actor Loss: 5.984267756886651e-08, Critic Loss: 1.4592497514697607\n",
      "Episode 18: Actor Loss: 5.255315870478228e-08, Critic Loss: 1.4535181289707544\n",
      "Episode 19: Actor Loss: 4.617943456963758e-08, Critic Loss: 1.4491205804329366\n",
      "Episode 20: Actor Loss: 4.0597176642620524e-08, Critic Loss: 1.4457397993654013\n",
      "Episode 21: Actor Loss: 3.5704215655130154e-08, Critic Loss: 1.4431378149660303\n",
      "Episode 22: Actor Loss: 3.141403137618681e-08, Critic Loss: 1.441134245141875\n",
      "Episode 23: Actor Loss: 2.7652167808867034e-08, Critic Loss: 1.4395908542606048\n",
      "Episode 24: Actor Loss: 2.4353670493582058e-08, Critic Loss: 1.4384017041372135\n",
      "Episode 25: Actor Loss: 2.1461811954281982e-08, Critic Loss: 1.4374854387715459\n",
      "Episode 26: Actor Loss: 1.8926730760426834e-08, Critic Loss: 1.4367790026078\n",
      "Episode 27: Actor Loss: 1.6704657852528725e-08, Critic Loss: 1.436234355326742\n"
     ]
    }
   ],
   "source": [
    "train(actor, critic, actor_optimizer, critic_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_strategy(actor: ActorNetwork, file):\n",
    "    abc = ABC()\n",
    "    init_stats = abc.read_aiger(file)\n",
    "    assert len(init_stats) == num_features\n",
    "\n",
    "    state = torch.tensor(init_stats[:2] + [1] * (num_features - 2), dtype=torch.float)\n",
    "    init_stats = torch.tensor(init_stats, dtype=torch.float)\n",
    "\n",
    "    for i in range(50):\n",
    "        action, _, prob = select_action(actor, state)\n",
    "        print(prob)\n",
    "        action_to_be_taken = possible_actions[action]\n",
    "        print(f\"Taking action {action}\")\n",
    "        new_stats = action_to_be_taken(abc)\n",
    "        next_state = torch.tensor(new_stats, dtype=torch.float) / init_stats\n",
    "        next_state[0] = state[0]\n",
    "        next_state[1] = state[1]\n",
    "\n",
    "        if init_stats[2] == 0:\n",
    "            next_state[2] = 0\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    print(state)\n",
    "    if abc.quit() != 0:\n",
    "        print(\"Error in quitting abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_strategy(actor, \"./benchmarks/arithmetic/adder.aig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abc = ABC()\n",
    "# init_stats = abc.read_aiger(\"i10.aig\")\n",
    "# assert len(init_stats) == num_features\n",
    "\n",
    "# state = torch.tensor(init_stats[:2] + [1] * (num_features - 2), dtype=torch.float)\n",
    "# init_stats = torch.tensor(init_stats, dtype=torch.float)\n",
    "# total_actor_loss = 0\n",
    "# total_critic_loss = 0\n",
    "\n",
    "# action_probs = actor(state)\n",
    "# action_distribution = Categorical(action_probs)\n",
    "# action = action_distribution.sample()\n",
    "# action_log_prob = action_distribution.log_prob(action).reshape(1)\n",
    "# action_to_be_taken = possible_actions[action]\n",
    "# new_stats = action_to_be_taken(abc)\n",
    "# next_state = torch.tensor(new_stats, dtype=torch.float) / init_stats\n",
    "# next_state[0] = state[0]\n",
    "# next_state[1] = state[1]\n",
    "\n",
    "# if init_stats[2] == 0:\n",
    "#     next_state[2] = 0\n",
    "# print(next_state)\n",
    "\n",
    "# abc.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
