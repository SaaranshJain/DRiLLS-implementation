{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Imports -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from abc_py.interface import ABC\n",
    "import functools\n",
    "import os\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Definitions -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_actions = [\n",
    "    functools.partial(ABC.resub, zero_cost=False, preserve_levels=False),\n",
    "    functools.partial(ABC.resub, zero_cost=False, preserve_levels=True),\n",
    "    functools.partial(ABC.resub, zero_cost=True, preserve_levels=False),\n",
    "    functools.partial(ABC.resub, zero_cost=True, preserve_levels=True),\n",
    "    functools.partial(ABC.rewrite, zero_cost=False, preserve_levels=True, verbose=False),\n",
    "    functools.partial(ABC.rewrite, zero_cost=True, preserve_levels=True, verbose=False),\n",
    "    functools.partial(ABC.rewrite, zero_cost=False, preserve_levels=False, verbose=False),\n",
    "    functools.partial(ABC.rewrite, zero_cost=True, preserve_levels=False, verbose=False),\n",
    "    functools.partial(ABC.refactor, zero_cost=False, preserve_levels=True),\n",
    "    functools.partial(ABC.refactor, zero_cost=False, preserve_levels=False),\n",
    "    functools.partial(ABC.refactor, zero_cost=True, preserve_levels=True),\n",
    "    functools.partial(ABC.refactor, zero_cost=True, preserve_levels=False),\n",
    "    functools.partial(ABC.balance),\n",
    "]\n",
    "\n",
    "num_actions = len(possible_actions)\n",
    "num_features = 4\n",
    "learning_rate = 0.01\n",
    "discount_factor = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, num_features, num_actions):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_features, 20)\n",
    "        self.fc2 = nn.Linear(20, 20)\n",
    "        self.fc3 = nn.Linear(20, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.softmax(self.fc3(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_features, 10)\n",
    "        self.fc2 = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(actor: ActorNetwork, state: torch.Tensor):\n",
    "    action_probs = actor(state)\n",
    "    action_distribution = Categorical(action_probs)\n",
    "    action = action_distribution.sample()\n",
    "    return action.item(), action_distribution.log_prob(action).reshape(1), action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_losses(critic: CriticNetwork, episode_design_states, optimisation_sequence, episode_rewards):\n",
    "    actor_loss = torch.tensor([0.0], dtype=torch.float)\n",
    "    critic_loss = torch.tensor([0.0], dtype=torch.float)\n",
    "\n",
    "    for i in range(len(episode_design_states) - 1):\n",
    "        state = episode_design_states[i]\n",
    "        next_state = episode_design_states[i + 1]\n",
    "        _, action_log_prob = optimisation_sequence[i]\n",
    "        reward = episode_rewards[i]\n",
    "\n",
    "        value = critic(state)\n",
    "        next_value = critic(next_state)\n",
    "        td_error = reward + discount_factor * next_value - value\n",
    "        actor_loss += -action_log_prob * td_error.detach()\n",
    "        critic_loss += td_error ** 2\n",
    "\n",
    "    return actor_loss, critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reward(area, delay, new_area, new_delay):\n",
    "    if new_delay < delay:\n",
    "        if new_area < area:\n",
    "            return 3\n",
    "        if new_area > area:\n",
    "            return 1\n",
    "        return 2\n",
    "    if new_delay > delay:\n",
    "        if new_area < area:\n",
    "            return -1\n",
    "        if new_area > area:\n",
    "            return -3\n",
    "        return -2\n",
    "    if new_area < area:\n",
    "        return 3\n",
    "    if new_area > area:\n",
    "        return -2\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(synth_rewards: list, discount_factor: float):\n",
    "    synth_rewards = torch.tensor(synth_rewards, dtype=torch.float)\n",
    "    discounted_rewards = torch.zeros_like(synth_rewards)\n",
    "    running_add = 0\n",
    "    for i in range(len(synth_rewards)):        \n",
    "        running_add = running_add * discount_factor + synth_rewards[i]\n",
    "        discounted_rewards[i] = running_add\n",
    "    return discounted_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(actor: ActorNetwork, critic: CriticNetwork, actor_optimizer, critic_optimizer, episodes=50, iterations=50):\n",
    "#     for episode in range(episodes):\n",
    "#         total_actor_loss = 0\n",
    "#         total_critic_loss = 0\n",
    "\n",
    "#         for dir in os.listdir(\"./benchmarks\"):\n",
    "#             if dir != \"arithmetic\":\n",
    "#                 continue\n",
    "            \n",
    "#             for filename in os.listdir(f\"./benchmarks/{dir}\"):\n",
    "#                 if not filename.endswith(\".aig\"):\n",
    "#                     continue\n",
    "\n",
    "#                 abc = ABC()\n",
    "#                 abc.read_aiger(f\"./benchmarks/{dir}/{filename}\")\n",
    "#                 init_stats = abc.read_libraries(\"libraries/asap7sc7p5t_SIMPLE_RVT_FF_nldm_201020.lib\", \"libraries/asap7sc7p5t_INVBUF_RVT_FF_nldm_201020.lib\")\n",
    "#                 assert len(init_stats) - 4 == num_features\n",
    "\n",
    "#                 state = torch.tensor([1] * num_features, dtype=torch.float)\n",
    "#                 area, delay = init_stats[6], init_stats[7]\n",
    "#                 init_stats = torch.tensor(init_stats[2:6], dtype=torch.float)\n",
    "                \n",
    "#                 for i in range(iterations):\n",
    "#                     # select action from actor model\n",
    "#                     action, action_log_prob, _ = select_action(actor, state)\n",
    "\n",
    "#                     # take action and observe next state\n",
    "#                     action_to_be_taken = possible_actions[action]\n",
    "#                     new_stats = action_to_be_taken(abc)\n",
    "#                     next_state = torch.tensor(new_stats[2:6], dtype=torch.float) / init_stats # take ratio with respect to initial stats\n",
    "\n",
    "#                     if init_stats[0] == 0:\n",
    "#                         next_state[0] = 0\n",
    "\n",
    "#                     # calculate reward and update actor and critic models\n",
    "#                     reward = calculate_reward(area, delay, new_stats[6], new_stats[7])\n",
    "#                     actor_loss, critic_loss = calculate_losses(critic, action_log_prob, reward, state, next_state)\n",
    "#                     total_actor_loss += actor_loss.detach().item()\n",
    "#                     total_critic_loss += critic_loss.detach().item()\n",
    "\n",
    "#                     actor_optimizer.zero_grad()\n",
    "#                     actor_loss.backward()\n",
    "#                     actor_optimizer.step()\n",
    "#                     critic_optimizer.zero_grad()\n",
    "#                     critic_loss.backward()\n",
    "#                     critic_optimizer.step()\n",
    "\n",
    "#                     # update state\n",
    "#                     state = next_state\n",
    "#                     area = new_stats[6]\n",
    "#                     delay = new_stats[7]\n",
    "\n",
    "#         total_actor_loss /= iterations\n",
    "#         total_critic_loss /= iterations\n",
    "#         print(f\"Episode {episode + 1}: Actor Loss: {total_actor_loss}, Critic Loss: {total_critic_loss}\")\n",
    "        \n",
    "#         if abc.quit() != 0:\n",
    "#             print(\"Error in quitting abc\")\n",
    "#             return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(actor: ActorNetwork, critic: CriticNetwork, actor_optimizer, critic_optimizer, episodes=50, iterations=50):\n",
    "    fileslist = os.listdir(\"./benchmarks/arithmetic\")\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        total_actor_loss = 0\n",
    "        total_critic_loss = 0\n",
    "\n",
    "        for filename in fileslist:\n",
    "            if not filename.endswith(\".aig\"):\n",
    "                continue\n",
    "\n",
    "            episode_design_states = []\n",
    "            optimisation_sequence = []\n",
    "            synth_rewards = []\n",
    "\n",
    "            abc = ABC()\n",
    "            abc.read_aiger(f\"./benchmarks/arithmetic/{filename}\")\n",
    "            init_stats = abc.read_libraries(\"libraries/asap7sc7p5t_SIMPLE_RVT_FF_nldm_201020.lib\", \"libraries/asap7sc7p5t_INVBUF_RVT_FF_nldm_201020.lib\")\n",
    "            assert len(init_stats) - 4 == num_features\n",
    "\n",
    "            state = torch.tensor([1] * num_features, dtype=torch.float)\n",
    "            area, delay = init_stats[6], init_stats[7]\n",
    "            init_stats = torch.tensor(init_stats[2:6], dtype=torch.float)\n",
    "            \n",
    "            for i in range(iterations):\n",
    "                # select action from actor model\n",
    "                action, action_log_prob, _ = select_action(actor, state)\n",
    "\n",
    "                # take action and observe next state\n",
    "                action_to_be_taken = possible_actions[action]\n",
    "                new_stats = action_to_be_taken(abc)\n",
    "                next_state = torch.tensor(new_stats[2:6], dtype=torch.float) / init_stats # take ratio with respect to initial stats\n",
    "\n",
    "                if init_stats[0] == 0:\n",
    "                    next_state[0] = 0\n",
    "\n",
    "                # calculate reward and update actor and critic models\n",
    "                reward = calculate_reward(area, delay, new_stats[6], new_stats[7])\n",
    "                episode_design_states.append(state)\n",
    "                optimisation_sequence.append((action, action_log_prob))\n",
    "                synth_rewards.append(reward)\n",
    "\n",
    "                # update state\n",
    "                state = next_state\n",
    "                area = new_stats[6]\n",
    "                delay = new_stats[7]\n",
    "\n",
    "            episode_rewards = discount_rewards(synth_rewards, discount_factor)\n",
    "            actor_loss, critic_loss = calculate_losses(critic, episode_design_states, optimisation_sequence, episode_rewards)\n",
    "            print(actor_loss.detach(), critic_loss.detach())\n",
    "            total_actor_loss += actor_loss.detach()\n",
    "            total_critic_loss += critic_loss.detach()\n",
    "\n",
    "            # propagate loss and step optmisers\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "\n",
    "        print(f\"Episode {episode + 1}: Actor Loss: {total_actor_loss / (len(fileslist) * iterations)}, Critic Loss: {total_critic_loss / (len(fileslist) * iterations)}\")\n",
    "        \n",
    "        if abc.quit() != 0:\n",
    "            print(\"Error in quitting abc\")\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = ActorNetwork(num_features, num_actions)\n",
    "critic = CriticNetwork(num_features)\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=learning_rate)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([523.9854]) tensor([1653.1078])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic_optimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 31\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(actor, critic, actor_optimizer, critic_optimizer, episodes, iterations)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# take action and observe next state\u001b[39;00m\n\u001b[1;32m     30\u001b[0m action_to_be_taken \u001b[38;5;241m=\u001b[39m possible_actions[action]\n\u001b[0;32m---> 31\u001b[0m new_stats \u001b[38;5;241m=\u001b[39m \u001b[43maction_to_be_taken\u001b[49m\u001b[43m(\u001b[49m\u001b[43mabc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m next_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(new_stats[\u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m6\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat) \u001b[38;5;241m/\u001b[39m init_stats \u001b[38;5;66;03m# take ratio with respect to initial stats\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m init_stats[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/drills/abc_py/interface.py:171\u001b[0m, in \u001b[0;36mABC.refactor\u001b[0;34m(self, preserve_levels, zero_cost)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_writeline(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrefactor\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcommand\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_training:\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/drills/abc_py/interface.py:41\u001b[0m, in \u001b[0;36mABC.print_stats\u001b[0;34m(self, write)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_readline()\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_readline()\n\u001b[0;32m---> 41\u001b[0m txt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_re\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_readline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     42\u001b[0m matches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats_re\u001b[38;5;241m.\u001b[39mmatch(txt)\n\u001b[1;32m     43\u001b[0m stats \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mint\u001b[39m(matches\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m)), \u001b[38;5;66;03m# primary inputs\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mint\u001b[39m(matches\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m2\u001b[39m)), \u001b[38;5;66;03m# primary outputs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mfloat\u001b[39m(matches\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m7\u001b[39m))  \u001b[38;5;66;03m# delay\u001b[39;00m\n\u001b[1;32m     52\u001b[0m ]\n",
      "File \u001b[0;32m~/Documents/drills/abc_py/interface.py:23\u001b[0m, in \u001b[0;36mABC._readline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_readline\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(actor, critic, actor_optimizer, critic_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_strategy(actor: ActorNetwork, file):\n",
    "#     abc = ABC()\n",
    "#     abc.read_aiger(file)\n",
    "#     assert len(init_stats) == num_features\n",
    "\n",
    "#     state = torch.tensor(init_stats[:2] + [1] * (num_features - 2), dtype=torch.float)\n",
    "#     init_stats = torch.tensor(init_stats, dtype=torch.float)\n",
    "\n",
    "#     for i in range(50):\n",
    "#         action, _, prob = select_action(actor, state)\n",
    "#         print(prob)\n",
    "#         action_to_be_taken = possible_actions[action]\n",
    "#         print(f\"Taking action {action}\")\n",
    "#         new_stats = action_to_be_taken(abc)\n",
    "#         next_state = torch.tensor(new_stats, dtype=torch.float) / init_stats\n",
    "#         next_state[0] = state[0]\n",
    "#         next_state[1] = state[1]\n",
    "\n",
    "#         if init_stats[2] == 0:\n",
    "#             next_state[2] = 0\n",
    "\n",
    "#         state = next_state\n",
    "\n",
    "#     print(state)\n",
    "#     if abc.quit() != 0:\n",
    "#         print(\"Error in quitting abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_strategy(actor, \"./benchmarks/arithmetic/adder.aig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abc = ABC()\n",
    "# abc.read_aiger(\"i10.aig\")\n",
    "# init_stats = abc.read_libraries(\"libraries/asap7sc7p5t_SIMPLE_RVT_FF_nldm_201020.lib\", \"libraries/asap7sc7p5t_INVBUF_RVT_FF_nldm_201020.lib\")\n",
    "# assert len(init_stats) - 4 == num_features\n",
    "\n",
    "# state = torch.tensor([1] * num_features, dtype=torch.float)\n",
    "# area, delay = init_stats[6], init_stats[7]\n",
    "# init_stats = torch.tensor(init_stats[2:6], dtype=torch.float)\n",
    "\n",
    "# total_actor_loss = 0\n",
    "# total_critic_loss = 0\n",
    "\n",
    "# action_probs = actor(state)\n",
    "# action_distribution = Categorical(action_probs)\n",
    "# action = action_distribution.sample()\n",
    "# action_log_prob = action_distribution.log_prob(action).reshape(1)\n",
    "\n",
    "# action_to_be_taken = possible_actions[action]\n",
    "# new_stats = action_to_be_taken(abc)\n",
    "# next_state = torch.tensor(new_stats[2:6], dtype=torch.float) / init_stats\n",
    "\n",
    "# reward = calculate_reward(area, delay, new_stats[6], new_stats[7])\n",
    "# print(reward)\n",
    "\n",
    "# abc.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
